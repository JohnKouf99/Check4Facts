{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../scripts')  \n",
    "from models import *\n",
    "from df_handling import *\n",
    "from search import *\n",
    "import pandas as pd\n",
    "from harvester import *\n",
    "from text_embedding import *\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer  = SummaryWriter(log_dir='new_runs_4.9')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting seeds for the classification task\n",
    "seed=42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float64)  # Convert arrays to PyTorch tensors\n",
    "        self.y = torch.tensor(y, dtype=torch.float64)    # Assuming y contains integer labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def count_layers(model):\n",
    "    \"\"\"Count the number of layers in a PyTorch model.\"\"\"\n",
    "    return sum(1 for _ in model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59288450\n"
     ]
    }
   ],
   "source": [
    "#neural network for n=3, model 5\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(6144+768*3,4608).double()\n",
    "        self.dropout1 = nn.Dropout(p=0.2)\n",
    "        self.fc2 = nn.Linear(4608,3072).double()\n",
    "        self.dropout2 = nn.Dropout(p=0.2)\n",
    "        self.fc3 = nn.Linear(3072,1536).double()\n",
    "        self.dropout3 = nn.Dropout(p=0.2)\n",
    "        self.fc4 = nn.Linear(1536,768).double()\n",
    "        self.dropout4 = nn.Dropout(p=0.2)\n",
    "        self.fc5 = nn.Linear(768,384).double()\n",
    "        self.dropout5 = nn.Dropout(p=0.2)\n",
    "        self.fc6 = nn.Linear(384,2).double()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.dropout4(x)\n",
    "        x = self.relu(self.fc5(x))\n",
    "        x = self.dropout5(x)\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "\n",
    "device  = (\"cuda\" if torch.cuda.is_available() else \"cpu\")   \n",
    "model = NeuralNetwork()\n",
    "model.to(device)\n",
    "\n",
    "#store the number of parameters for the model\n",
    "num_parameters = count_parameters(model)\n",
    "print(num_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr=0.01, weight_decay=10: \n",
      "Accuracy of the network on the n=3: 70.49180327868852 %\n"
     ]
    }
   ],
   "source": [
    "# n=1 feature classification\n",
    "num_epochs = 10\n",
    "batch_size = 128 \n",
    "num_folds = 10\n",
    "best_val_loss = np.Inf\n",
    "min_delta = 0.0001\n",
    "patience = 5\n",
    "num_folds=5\n",
    "kf = KFold(n_splits=num_folds)\n",
    "pca1 = PCA(n_components=1)\n",
    "greek2 = pd.read_csv('../data/sen_greek3.csv')\n",
    "greek2 = unravel_df(greek2, ['text_embedding_claim','text_embedding_par', 'text_embedding_sen'])\n",
    "cyprus2 = pd.read_csv('../data/sen_cyprus3.csv')\n",
    "cyprus2 = unravel_df(cyprus2, ['text_embedding_claim','text_embedding_par', 'text_embedding_sen'])\n",
    "test_df2 = pd.read_csv('../data/sen_check4facts3.csv')\n",
    "test_df2 = unravel_df(test_df2, ['text_embedding_claim','text_embedding_par', 'text_embedding_sen'])\n",
    "\n",
    "\n",
    "#we proceed to create the new dataframe\n",
    "test_df2['claim_id'] += int(greek2['claim_id'].iloc[-1]) +1\n",
    "cyprus2['claim_id'] += int(test_df2['claim_id'].iloc[-1]) +1\n",
    "df_train = pd.concat([greek2,test_df2,cyprus2], ignore_index=True)\n",
    "df_train = df_train[df_train.label<=1]\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "combined_array = list()\n",
    "label = list()\n",
    "for i in df_train.claim_id.unique():\n",
    "    df = df_train[df_train.claim_id==i]\n",
    "\n",
    "    claim=np.array(df.iloc[0,8])\n",
    "    label.append(df.iloc[0,9])\n",
    "\n",
    "    par= []\n",
    "    for j in range(len(df)):\n",
    "        par = np.concatenate([par,df.iloc[j,10]])\n",
    "        \n",
    "        sen = np.array(df.iloc[j,13], dtype=np.float64)\n",
    "        sen = sen.reshape(768,2)\n",
    "        sen = pca1.fit_transform(sen)\n",
    "        sen = np.squeeze(sen.reshape(1,-1))\n",
    "        par = np.concatenate([par,sen])\n",
    "\n",
    "\n",
    "    claim = np.concatenate([claim, par])\n",
    "    combined_array.append(np.ravel(claim))\n",
    "\n",
    "#creating training, testing and validation datasets\n",
    "arr = [combined_array[i] for i in range(len(combined_array))]\n",
    "\n",
    "#making a 80/10/10 train, test and validation split\n",
    "X_train, X_test, y_train, y_test= train_test_split(arr  ,np.array(label).astype('int'), test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "#oversampling method to add extra data for class balance\n",
    "smote = SMOTE(random_state=42, k_neighbors=4)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "#pca feature reduction \n",
    "# X_train_scaled, X_test_scaled, X_val_scaled = list(),list(),list()\n",
    "\n",
    "# for x_train in X_train:\n",
    "#     x_train = np.array(x_train, dtype=np.float64)\n",
    "#     x_train = x_train.reshape(768, 11)\n",
    "#     pca = PCA(n_components = 1) \n",
    "#     X_train_scaled.append(pca.fit_transform(x_train))\n",
    "    \n",
    "# for x_test in X_test:\n",
    "#     x_test = np.array(x_test, dtype=np.float64)\n",
    "#     x_test = x_test.reshape(768, 11)\n",
    "#     pca = PCA(n_components = 1) \n",
    "#     X_test_scaled.append(pca.fit_transform(x_test))\n",
    "\n",
    "\n",
    "# train_dataset = CustomDataset(np.squeeze(X_train_scaled), y_train)\n",
    "# test_dataset = CustomDataset(np.squeeze(X_test_scaled), y_test)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "test_dataset = CustomDataset(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "# # Create data loaders\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, worker_init_fn=seed_worker)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, worker_init_fn=seed_worker)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for learning_rate in [0.01]:\n",
    "    for w in  [10]:\n",
    "        print(f\"lr={learning_rate}, weight_decay={w}: \")\n",
    "        #loss and test loader\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, worker_init_fn=seed_worker)\n",
    "\n",
    "        #lists to save the best val/train loss on each fold\n",
    "        kfold_accs_val = list()\n",
    "        kfold_accs_train = list()\n",
    "        #storing the best val loss across all folds\n",
    "        best_val_loss_folds=np.Inf\n",
    "        for fold, (train_indices, val_indices) in enumerate(kf.split(train_dataset)):\n",
    "            # print(f\"Fold {fold + 1}, lr={learning_rate}, weight_decay={w}: \")\n",
    "\n",
    "            #optimizer, and data loader initialization\n",
    "            model = NeuralNetwork()\n",
    "            model.to(device)\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr= learning_rate, weight_decay=w) \n",
    "            train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, sampler=torch.utils.data.SubsetRandomSampler(train_indices))\n",
    "            val_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, sampler=torch.utils.data.SubsetRandomSampler(val_indices))\n",
    "            \n",
    "\n",
    "            #best_val_loss variable keeps the best validation score for each fold\n",
    "            best_val_loss = np.Inf\n",
    "          \n",
    "\n",
    "            #training\n",
    "            total_steps = len(train_loader)\n",
    "            for i in range(num_epochs):\n",
    "                model.train()\n",
    "                train_loss = 0.0\n",
    "                for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "                    #reshaping the training data passing the them to the gpu\n",
    "                    data = data.to(device)\n",
    "                    target = target.type(torch.LongTensor)\n",
    "                    target = target.to(device)\n",
    "                    \n",
    "\n",
    "                    #forward pass\n",
    "                    outputs = model(data)\n",
    "                    loss = criterion(outputs, target)\n",
    "\n",
    "                    #backward pass and optimize \n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    train_loss += loss.item()\n",
    "\n",
    "                # Compute average training loss for the epoch\n",
    "                avg_train_loss = train_loss / len(train_loader)  \n",
    "                \n",
    "            \n",
    "\n",
    "                #validation\n",
    "                model.eval()\n",
    "                val_loss = 0.0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    avg_val_loss=0\n",
    "                    for data, target in val_loader:\n",
    "                        target = target.type(torch.LongTensor)\n",
    "                        data, target = data.to(device), target.to(device)\n",
    "\n",
    "                        #compute validation loss\n",
    "                        outputs = model(data)\n",
    "                        loss_val = criterion(outputs,target)\n",
    "                        val_loss += loss_val.item()\n",
    "                        \n",
    "            \n",
    "                    #print validation and training loss     \n",
    "                    avg_val_loss = val_loss / len(val_loader)\n",
    "                    \n",
    "\n",
    "                    #early stopping condition\n",
    "                    if avg_val_loss<best_val_loss - min_delta:\n",
    "                        best_val_loss = avg_val_loss\n",
    "                        best_train_loss = avg_train_loss\n",
    "                        current_patience = 0\n",
    "                        #update best validation loss across all folds\n",
    "                        if best_val_loss< best_val_loss_folds-min_delta:\n",
    "                            best_val_loss_folds = best_val_loss\n",
    "                            torch.save(model.state_dict(), 'model.pth')\n",
    "                        \n",
    "                    else:\n",
    "                        current_patience +=1\n",
    "                        if current_patience>= patience:\n",
    "                            # print (f'''Epoch [{i+1}/{num_epochs}],  Training_loss: {avg_train_loss:.4f}, Validation_loss: {avg_val_loss:.4f},''')\n",
    "                            # print(f'Early stopping afther {i+1} epochs')\n",
    "                            break\n",
    "                    \n",
    "\n",
    "                \n",
    "                #print (f'''Epoch [{i+1}/{num_epochs}],  Training_loss: {avg_train_loss:.4f}, Validation_loss: {avg_val_loss:.4f},''')\n",
    "\n",
    "            #save the best validation loss and the corresponding training loss to this \n",
    "            kfold_accs_val.append(round(best_val_loss,3))\n",
    "            kfold_accs_train.append(round(best_train_loss,3))\n",
    "\n",
    "            #if(fold==num_folds-1):\n",
    "                # print(f\"KFold cross validation results: {kfold_accs_val}\")\n",
    "                # print(f\"KFold cross validation training results: {kfold_accs_train}\")\n",
    "                # print(f\"Validation loss mean {np.mean(kfold_accs_val)}\" )\n",
    "                # print(f\"Train loss mean {np.mean(kfold_accs_train)}\" )\n",
    "\n",
    "        #Log training and validation loss to tensorboard\n",
    "\n",
    "        # writer.add_scalars(f'Model Complexity (n=3), weight_decay={w}, for learning rate = {learning_rate}', {\n",
    "        #                                 'train': np.mean(kfold_accs_train),\n",
    "        #                                 'val': np.mean(kfold_accs_val),\n",
    "        #                             }, num_parameters)\n",
    "              \n",
    "        #testing\n",
    "        with torch.no_grad():\n",
    "            n_correct=0\n",
    "            n_samples=0\n",
    "            model.load_state_dict(torch.load('model.pth'))\n",
    "            model.eval()\n",
    "            for data, target in test_loader:\n",
    "\n",
    "                #reshaping the testing data passing the them to the gpu\n",
    "                data = data.to(device)\n",
    "                target = target.type(torch.LongTensor)\n",
    "                target = target.to(device)\n",
    "                outputs = model(data)\n",
    "\n",
    "                # max returns (value ,index)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                n_samples += target.size(0)\n",
    "                n_correct += (predicted == target).sum().item()\n",
    "\n",
    "            acc = 100.0 * n_correct / n_samples\n",
    "            print(f'Accuracy of the network on the n=3: {acc} %')\n",
    "\n",
    "            writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
